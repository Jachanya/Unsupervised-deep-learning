{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOp4X5oTAmFJqQlQnrq/DWV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jachanya/Unsupervised-deep-learning/blob/main/transformerLanguageModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "from typing import Sequence\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import sys,random,math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "AhBCqur5GT4R"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "pUPZxtgzDjgj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "I_DdG3lhEJny"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "LwLk1V2gGYyC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "PAD_token = 2\n",
        "UNK_token = 3\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name, filename):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"<SOS>\", 1: \"<EOS>\", 2: \"<PAD>\", 3: \"<UNK>\"}\n",
        "        self.n_words = 4  # Count SOS and EOS\n",
        "        self.make_vocab(filename)\n",
        "\n",
        "    def make_vocab(self,filename):\n",
        "      with open(filename, encoding = 'utf-8') as file:\n",
        "        new_file = file.read()\n",
        "        for i in range(len(new_file)//1000000):\n",
        "          self.addSentence(new_file[i*1000000:i*1000000 + 1000000])\n",
        "        #for sentence in file:\n",
        "        #  self.addSentence(sentence)\n",
        "      print('done')\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        doc = nlp(sentence)\n",
        "        for token in doc:\n",
        "          self.addWord(token.text.lower())\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def getWord(self, word):\n",
        "      if word not in self.word2index:\n",
        "        return UNK_token\n",
        "      \n",
        "      else:\n",
        "        return self.word2index[word]\n",
        "    \n",
        "    def getIndex(self, index):\n",
        "      return self.index2word[index]\n",
        "\n",
        "    def getSentence(self, sentence):\n",
        "      doc = nlp(sentence)\n",
        "      new_sentence = []\n",
        "      for token in doc:\n",
        "        new_sentence.append(self.getWord(token.text.lower()))\n",
        "      return new_sentence \n",
        "    \n",
        "    def getListIndex(self, index):\n",
        "      sentence = []\n",
        "      for idx in index:\n",
        "        sentence.append(self.getIndex(idx))\n",
        "      return sentence\n",
        "\n",
        "language = Lang('English', 'data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le2zdHUfIDei",
        "outputId": "81709fd7-f572-41b5-c807-7bc457e6ee8d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(language.index2word)"
      ],
      "metadata": {
        "id": "iY3y5y8YGexx"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Mfi_t7DY3-du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKA7W4D4WNZn",
        "outputId": "f8c14837-ada9-458c-e34f-34379a5aac3c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26215"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def generate_seq(filename, language):\n",
        "  with open(filename, encoding ='utf-8') as file:\n",
        "    sentences = []\n",
        "    for sentence in file.read().split(\"\\n\"):\n",
        "      #print(word)\n",
        "      if len(sentence.strip()) > 4:\n",
        "        sentences.append(torch.tensor(language.getSentence(sentence.strip())))\n",
        "  return sentences\n",
        "'''\n",
        "  "
      ],
      "metadata": {
        "id": "8bQJzgtTHE6Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0b882573-d9ec-473e-fb26-ed3bac5fe760"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef generate_seq(filename, language):\\n  with open(filename, encoding =\\'utf-8\\') as file:\\n    sentences = []\\n    for sentence in file.read().split(\"\\n\"):\\n      #print(word)\\n      if len(sentence.strip()) > 4:\\n        sentences.append(torch.tensor(language.getSentence(sentence.strip())))\\n  return sentences\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import random"
      ],
      "metadata": {
        "id": "9HUvnTwgoZ4a"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sequences = generate_seq('data/shakespeare.txt')\n"
      ],
      "metadata": {
        "id": "9IPhM2LyNMao"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Shakespeare(Dataset):\n",
        "  def __init__(self, dir, transform = None):\n",
        "    self.sequences = self.generate_seq(dir)\n",
        "    self.transform = transform\n",
        "    self.PAD_token = 2\n",
        "    self.EOS_token = 1\n",
        "    self.SOS_token = 0\n",
        "\n",
        "  def generate_seq(self, filename):\n",
        "    with open(filename, encoding ='utf-8') as file:\n",
        "      sentences = []\n",
        "      for sentence in file.read().split(\"\\n\"):\n",
        "        #print(word)\n",
        "        if len(sentence.strip()) > 4:\n",
        "          sentences.append(sentence.strip())\n",
        "    return sentences\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.sequences)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    #assert isinstance(idx, list)\n",
        "    out = random.sample(self.sequences, idx)\n",
        "    seq_out = []\n",
        "    for i in range(len(out)):\n",
        "      seq_out.append(torch.tensor(language.getSentence(out[i])))\n",
        "\n",
        "    out = pad_sequence(seq_out, padding_value = PAD_token).permute(1,0)\n",
        "    out = F.pad(out, (0, 1), value = EOS_token)\n",
        "    out = F.pad(out, (1, 0), value = SOS_token)\n",
        "    return out"
      ],
      "metadata": {
        "id": "tHXJfpL5mVKo"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Shakespeare(dir = 'data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "xvceb4CwQAgl"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(dataset, bs):\n",
        "  ds = len(dataset)\n",
        "  assert bs <= ds, 'Batch size must be less than or equal to input dim'\n",
        "  perm = torch.randperm(ds)\n",
        "  for i in range(ds//bs):\n",
        "    yield dataset[bs]"
      ],
      "metadata": {
        "id": "hu1zGCOEWDSU"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6N5ljDMzCwI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformer\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, heads, src_len, trg_len, d_model, vocab_size, n_blocks = 4):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.heads = heads\n",
        "        self.src_len = src_len\n",
        "        self.trg_len = trg_len\n",
        "        self.d_model = d_model\n",
        "        self.n_blocks = n_blocks\n",
        "        self.vocab_size = vocab_size \n",
        "\n",
        "        self.encoders = nn.ModuleList([Encoder(self.heads, self.src_len, self.d_model) for i in range(self.n_blocks)]) \n",
        "        self.decoders = nn.ModuleList([Decoder(self.heads, self.trg_len, self.d_model) for i in range(self.n_blocks)]) \n",
        "\n",
        "        self.pe = PositionalEncoding(self.src_len, self.d_model)\n",
        "        self.embed = nn.Embedding(self.vocab_size, self.d_model)\n",
        "\n",
        "        self.linear = nn.Linear(self.d_model, self.vocab_size)\n",
        "\n",
        "    def forward(self, src_idx, trg_idx):\n",
        "        bs = src_idx.size(0)\n",
        "        src_out = self.pe()[:self.src_len,:] + self.embed(src_idx)\n",
        "\n",
        "        for i in range(len(self.encoders)):\n",
        "            src_out = self.encoders[i](src_out)\n",
        "\n",
        "        trg_out = self.pe()[:self.trg_len,:] + self.embed(trg_idx)\n",
        "        for i in range(len(self.decoders)):\n",
        "            trg_out = self.decoders[i](trg_out, src_out)\n",
        "\n",
        "        trg_out = self.linear(trg_out)\n",
        "        #trg_out = F.softmax(trg_out, dim = -1)\n",
        "        #trg_out = trg_out.view(bs, self.trg_len, self.vocab_size)\n",
        "        return trg_out\n",
        "\n",
        "#class GPT\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self , heads, trg_len, d_model, vocab_size, n_blocks = 4):\n",
        "        super(GPT, self).__init__()\n",
        "\n",
        "        self.heads = heads\n",
        "        self.trg_len = trg_len\n",
        "        self.d_model = d_model\n",
        "        self.n_blocks = n_blocks\n",
        "        self.vocab_size = vocab_size \n",
        "\n",
        "        #self.encoders = nn.ModuleList([Encoder(self.heads, self.src_len, self.d_model) for i in range(self.n_blocks)]) \n",
        "        self.decoders = nn.ModuleList([Decoder(self.heads, self.trg_len, self.d_model) for i in range(self.n_blocks)]) \n",
        "\n",
        "        self.pe = PositionalEncoding(self.trg_len, self.d_model)\n",
        "        self.embed = nn.Embedding(self.vocab_size, self.d_model)\n",
        "\n",
        "        self.linear = nn.Linear(self.d_model, self.vocab_size)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        bs = idx.size(0)\n",
        "\n",
        "        trg_out = self.pe()[:idx.size(1),:] + self.embed(idx)\n",
        "\n",
        "        for i in range(len(self.decoders)):\n",
        "            trg_out = self.decoders[i](trg_out, trg_out)\n",
        "\n",
        "        trg_out = self.linear(trg_out)\n",
        "        #trg_out = F.softmax(trg_out, dim = -1)\n",
        "        #trg_out = trg_out.view(bs, self.trg_len, self.vocab_size)\n",
        "        return trg_out\n",
        "\n",
        "#Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, seq_len, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        \n",
        "        self.register_buffer('PE', self.generate_pe())\n",
        "\n",
        "    def forward(self):\n",
        "        return self.PE\n",
        "\n",
        "    def generate_pe(self):\n",
        "        pos_enc = torch.zeros((self.seq_len, self.d_model))\n",
        "        den = torch.pow(10000, 2 * (torch.arange(pos_enc.size(1)//2)) / self.d_model)\n",
        "        pos_enc[:, 0::2] = torch.sin(torch.arange(pos_enc.size(0)).unsqueeze(1)/den)\n",
        "        pos_enc[:, 1::2] = torch.cos(torch.arange(pos_enc.size(0)).unsqueeze(1)/den)\n",
        "        return pos_enc\n",
        "#Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, heads,seq_len, d_model):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.heads = heads\n",
        "        self.d_model = d_model\n",
        "        self.multihead_attn = MultiHeadAttention(self.heads, self.d_model)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ffnn = FFNN(self.d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(0.8)\n",
        "        self.dropout2 = nn.Dropout(0.8)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs = self.ln1(self.dropout1(self.multihead_attn(inputs, inputs, inputs)) + inputs)\n",
        "        inputs = self.ln2(self.dropout2(self.ffnn(inputs)) + inputs)\n",
        "        return inputs\n",
        "\n",
        "#Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, heads, trg_len, d_model):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.heads = heads\n",
        "        self.d_model = d_model\n",
        "        self.trg_len = trg_len \n",
        "\n",
        "        self.mask_attn = MultiHeadAttention(self.heads, self.d_model)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.multihead_attn = MultiHeadAttention(self.heads, self.d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.ffnn = FFNN(self.d_model)\n",
        "        self.ln3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.8)\n",
        "        self.dropout2 = nn.Dropout(0.8)\n",
        "        self.dropout3 = nn.Dropout(0.8)\n",
        "        #self.generate_mask()\n",
        "\n",
        "    def forward(self, inputs, keys):\n",
        "\n",
        "        inputs = self.ln1(self.dropout1(self.multihead_attn(inputs, inputs, inputs, mask = self.generate_mask(inputs.size(1))) + inputs))\n",
        "        inputs = self.ln2(self.dropout2(self.multihead_attn(inputs, keys, keys)) + inputs)\n",
        "        inputs = self.ln3(self.dropout3(self.ffnn(inputs)) + inputs)\n",
        "        return inputs\n",
        "\n",
        "    def generate_mask(self, trg_len):\n",
        "        mask = torch.ones((trg_len, trg_len))\n",
        "        mask = torch.triu(mask, diagonal = 1) * -100000\n",
        "        self.register_buffer('mask', mask)\n",
        "        return mask\n",
        "#FFNN\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super(FFNN, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.ln1 = nn.Linear(self.d_model, self.d_model * 4)\n",
        "        self.ln2 = nn.Linear(self.d_model * 4, self.d_model)\n",
        "        self.lnorm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        inputs = F.relu(self.ln1(x))\n",
        "        inputs = self.lnorm(self.ln2(inputs) + x)\n",
        "        return inputs\n",
        "\n",
        "#Multihead Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.heads = heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert self.d_model % self.heads == 0, 'heads has to be a multiple of d_model'\n",
        "        self.query = nn.Linear(self.d_model, self.d_model, bias = False)\n",
        "        self.key = nn.Linear(self.d_model, self.d_model, bias = False)\n",
        "        self.value = nn.Linear(self.d_model, self.d_model, bias = False)\n",
        "\n",
        "        self.ln = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        bs = query.size(0)\n",
        "        seq_len = query.size(1)\n",
        "\n",
        "        #print(query.shape)\n",
        "        #inputs = inputs.view(-1, self.d_model)\n",
        "        #print(self.query(query).shape)\n",
        "        query = self.query(query).view(bs, seq_len, self.heads, self.d_model//self.heads)\n",
        "        key = self.key(key).view(bs, seq_len, self.heads, self.d_model//self.heads)\n",
        "        value = self.value(value).view(bs, seq_len, self.heads, self.d_model//self.heads)\n",
        "\n",
        "        \n",
        "        #print(query.shape)\n",
        "        query = query.permute(0,2,1,3).contiguous().view(bs * self.heads, seq_len, self.d_model//self.heads)\n",
        "        key = key.permute(0,2,1,3).contiguous().view(bs * self.heads, seq_len, self.d_model//self.heads)\n",
        "        value = value.permute(0,2,1,3).contiguous().view(bs * self.heads, seq_len, self.d_model//self.heads)\n",
        "\n",
        "        inputs = sccaled_dot_product_attention(query, key, value, mask) \n",
        "        inputs = inputs.view(bs, self.heads, seq_len, self.d_model//self.heads).permute(0,2,1,3).contiguous()\n",
        "        inputs = inputs.view(bs, seq_len, self.heads * self.d_model//self.heads)\n",
        "        inputs = self.ln(inputs)\n",
        "        return inputs\n",
        "\n",
        "#scaled dot product attention\n",
        "def sccaled_dot_product_attention(query, key, value, mask):\n",
        "    if mask is not None:\n",
        "        attn = torch.bmm(query, key.transpose(-1,-2))/torch.sqrt(torch.FloatTensor([key.size(-1)]).to(key.device)) + mask.to(key.device).detach()\n",
        "    else:\n",
        "        attn = torch.bmm(query, key.transpose(-1,-2))/torch.sqrt(torch.FloatTensor([key.size(-1)]).to(key.device))\n",
        "\n",
        "    attn = torch.softmax(attn, dim = -1)\n",
        "    \n",
        "    output = torch.bmm(attn, value)\n",
        "    return output"
      ],
      "metadata": {
        "id": "3Qs7kf5W1Q02"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions.categorical import Categorical\n",
        "def generate_(model, seq_len, language):\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    seq = [SOS_token]\n",
        "    for i in range(seq_len):\n",
        "      out = model(torch.tensor(seq).unsqueeze(1).to('cuda'))\n",
        "      out = F.softmax(out, dim = -1)\n",
        "      m = Categorical(out)\n",
        "      token = m.sample().squeeze(1).tolist()[-1]\n",
        "      #token = torch.argmax(out, dim = -1).squeeze(1).tolist()[-1]\n",
        "      if token == 1:\n",
        "        break\n",
        "      if token not in [0,1,2]:\n",
        "        seq.append(token)\n",
        "    return language.getListIndex(seq[1:])"
      ],
      "metadata": {
        "id": "4MnIbwvHIB4L"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(8, 256, 800, vocab_size, n_blocks = 6).to('cuda')\n",
        "model = torch.load('model_weights.pth')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index = PAD_token)"
      ],
      "metadata": {
        "id": "sw_tb91MjHCW"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "batch_size = 15\n",
        "for i in range(100):\n",
        "    total_loss = 0.0\n",
        "    den = 0.0\n",
        "    for data in get_batch(dataset, batch_size):\n",
        "        model.train()\n",
        "        input_ = data[:, :-1].to('cuda')\n",
        "        target_ = data[:, 1:].to('cuda')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = model(input_)\n",
        "        #print(out.view(-1, vocab_size))\n",
        "        #print(target_.view(-1))\n",
        "        loss = loss_fn(out.view(-1, vocab_size), target_.view(-1))\n",
        "\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        #print(loss.item())\n",
        "        den += input_.size(1)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    if i % 10 == 0 or i == 0:\n",
        "        print(total_loss / den)\n",
        "        losses.append(total_loss / den)\n",
        "        torch.save(model, 'model_weights.pth')\n",
        "        print(' '.join(generate_(model, 100, language)))\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "ljpe82_pq-h1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.save(model, 'model_weights.pth')\n",
        "print(' '.join(generate_(model, 10, language)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w35JiWI9Hfdr",
        "outputId": "42113a12-dd59-4d65-b0ba-26dea81177a3"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is gain will . . yokefellow heart is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZqIk3oS1jMU1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}