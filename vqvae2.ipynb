{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residual Connection\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Residual, self).__init__()\n",
    "        self.model = nn.Sequential(nn.BatchNorm2d(dim),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Conv2d(dim, dim, kernel_size = 3, stride = 1, padding = 1),\n",
    "                                  nn.BatchNorm2d(dim),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Conv2d(dim, dim, kernel_size = 1, stride = 1, padding = 0),)\n",
    "    def forward(self, x):\n",
    "        x = self.model(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_of_blocks, dim):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self._num_of_blocks = num_of_blocks\n",
    "        self.model = nn.ModuleList([Residual(dim) for i in range(self._num_of_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for model in self.model:\n",
    "            x = model(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder\n",
    "'''\n",
    "input -> NCHW\n",
    "multiple output depending on the hierarchy\n",
    "'''\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, num_of_blocks, factor = 4):\n",
    "        super(Encoder, self).__init__()\n",
    "        assert factor in [2, 4], 'Factor has to be either 1 or 2'\n",
    "        \n",
    "        if factor == 4:\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Conv2d(input_dim, out_dim, kernel_size = 4, stride = 2, padding = 1),\n",
    "                nn.BatchNorm2d(out_dim),\n",
    "                nn.ReLU(inplace = True),\n",
    "                nn.Conv2d(out_dim, out_dim, kernel_size = 4, stride = 2, padding = 1),\n",
    "            )\n",
    "        \n",
    "        elif factor == 2:\n",
    "            self.model = nn.Sequential(\n",
    "                 nn.Conv2d(input_dim, out_dim, kernel_size = 4, stride = 2, padding = 1),\n",
    "            )\n",
    "            \n",
    "        self.res1 = ResBlock(num_of_blocks, out_dim)\n",
    "        self.res2 = ResBlock(num_of_blocks, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        #print(x_bottom.shape)\n",
    "        x = self.res2(self.res1(x))\n",
    "        #print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quantized\n",
    "class Quantized(nn.Module):\n",
    "    def __init__(self, num_embeddings, embed_dim, commitment_cost = 0.25):\n",
    "        super(Quantized, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.embeddings = nn.Embedding(self.num_embeddings, self.embed_dim)\n",
    "        self.embeddings.weight.data.uniform_(-1./self.num_embeddings, 1./self.num_embeddings)\n",
    "        self._commitment_cost = commitment_cost\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        input -> NCHW\n",
    "        '''\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = x.shape\n",
    "        x_flat = x.reshape(-1, self.embed_dim)\n",
    "        \n",
    "        distances =  (torch.sum(x_flat ** 2, dim = 1, keepdim = True)\n",
    "                     + torch.sum(self.embeddings.weight ** 2, dim = 1)\n",
    "                     - 2 * (torch.matmul(x_flat, self.embeddings.weight.t())))\n",
    "\n",
    "        #quantized = self.embeddings(torch.argmin(distance, dim = 1)).reshape(input_shape)\n",
    "        \n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings, device=x.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        \n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self.embeddings.weight).view(input_shape)\n",
    "        \n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), x)\n",
    "        q_latent_loss = F.mse_loss(quantized, x.detach())\n",
    "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
    "        \n",
    "        quantized = x + (quantized - x).detach()\n",
    "        \n",
    "        return loss, quantized.permute(0, 3, 1, 2).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, num_of_blocks, factor = 4):\n",
    "        super(Decoder, self).__init__()\n",
    "        assert factor in [2, 4], 'Factor has to be either 2 or 4'\n",
    "        \n",
    "        self.res1 = ResBlock(num_of_blocks, input_dim)\n",
    "        self.res2 = ResBlock(num_of_blocks, input_dim)\n",
    "        \n",
    "        if factor == 4:\n",
    "            self.model = nn.Sequential(\n",
    "                nn.BatchNorm2d(input_dim),\n",
    "                nn.ReLU(inplace = True),\n",
    "                nn.ConvTranspose2d(input_dim, input_dim, kernel_size = 4, stride = 2, padding = 1),\n",
    "                nn.BatchNorm2d(input_dim),\n",
    "                nn.ReLU(inplace = True),\n",
    "                nn.ConvTranspose2d(input_dim, out_dim, kernel_size = 4, stride = 2, padding = 1),\n",
    "            )\n",
    "        \n",
    "        elif factor == 2:\n",
    "            self.model = nn.Sequential(\n",
    "                nn.BatchNorm2d(input_dim),\n",
    "                nn.ReLU(inplace = True),\n",
    "                nn.ConvTranspose2d(input_dim, out_dim, kernel_size = 4, stride = 2, padding = 1),\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.res2(self.res1(x))\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VQ-VAE2\n",
    "class VQVAE2(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, num_of_blocks,num_embeddings, embed_dim):\n",
    "        super(VQVAE2, self).__init__()\n",
    "        self.enc_b = Encoder(input_dim, out_dim//2, num_of_blocks, factor = 4)\n",
    "        self.enc_t = Encoder(out_dim//2, out_dim, num_of_blocks, factor = 2)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(out_dim, embed_dim, kernel_size = 1, stride = 1, padding = 0)\n",
    "        \n",
    "        self.qt_t = Quantized(num_embeddings, embed_dim)\n",
    "        \n",
    "        self.dec_t = Decoder(out_dim, out_dim//2, num_of_blocks, factor = 2)\n",
    "        self.conv2 = nn.Conv2d(out_dim//2 + out_dim//2, embed_dim, kernel_size = 1, stride = 1, padding = 0)\n",
    "           \n",
    "        self.qt_b = Quantized(num_embeddings, embed_dim)\n",
    "        \n",
    "        self.dec_b = Decoder(out_dim//2, input_dim, num_of_blocks, factor = 4)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(embed_dim + out_dim//2, out_dim//2, kernel_size = 1, stride = 1, padding = 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        quant_t, quant_b, loss = self.encode(x)\n",
    "        out = self.decode(quant_t, quant_b)\n",
    "        return loss, out\n",
    "    \n",
    "    def encode(self, x):\n",
    "        enc_b = self.enc_b(x)\n",
    "        enc_t = self.enc_t(enc_b)\n",
    "        \n",
    "        quant_t = self.conv1(enc_t)\n",
    "        loss_1, quant_t = self.qt_t(quant_t)\n",
    "        #print(quant_t.shape)\n",
    "        dec_t = self.dec_t(quant_t)\n",
    "        #print(dec_t.shape)\n",
    "        #print(enc_b.shape)\n",
    "        enc_b = torch.cat([dec_t, enc_b], 1)\n",
    "        #print(enc_b.shape)\n",
    "        quant_b = self.conv2(enc_b)\n",
    "        #print(quant_b.shape)\n",
    "        loss_2, quant_b = self.qt_b(quant_b)\n",
    "        \n",
    "        return quant_t, quant_b, loss_1 + loss_2\n",
    "    \n",
    "    def decode(self, quant_t, quant_b):\n",
    "        dec_t = self.dec_t(quant_t)\n",
    "        quant = torch.cat([quant_b, dec_t], dim = 1)\n",
    "        quant = self.conv3(quant)\n",
    "        dec = self.dec_b(quant)\n",
    "        return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQVAE2(3, 256,20,512, 256).to('cuda')\n",
    "data = torch.randn(1, 3, 32,32).to('cuda')\n",
    "loss, out = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GatedPixelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
